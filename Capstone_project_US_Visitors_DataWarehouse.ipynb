{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Wrehouse: US Visitors \n",
    "__Provide insights to the officials' decision-making towards optimizing the experience of the visitors in the US__\n",
    "***\n",
    "## Project overview\n",
    "The project is the last project og the Data Engineering Nanodegree in Udacity. The aim of the capstone project is to provide the students of the Nanodegree a chance to demonstrate the knowledge gained throught the courses of the program. This final project will be a critical part of students portfolio that will help to achieve data engineering-related career goals. There were two options available to complete this project one is the data provided by the Udacity team or the student need to describe the data and scope ourselves.  I choose the data provided by Udacity to build the Data Warehouse. The aforementioned data are on immigration to the United States and are provided by the US National Tourism and Trade Office. For more informations please visit https://travel.trade.gov/research/reports/i94/historical/2016.html\n",
    "\n",
    "## Business Scenario\n",
    "As a Data Engineer in the InsightData, a data oriented tech firm specialized in data warehouse services. We provided exclusive assistance to enterprises navigating their data needs and we create smart and innovative strategic solutions that deliver optimized business results. In total, our full suite of services includes modernization of businesses data warehousing infrastructure, improvement of the performance and ease of use for the end users of the DW, decrease of cost, helping with data profiling and standardization, data acquisition, transformation and integration.\n",
    "\n",
    "The U.S. Customs and Border Protection hired us as contractors to provide them with hidden insight of the data flood. Our goal is to model and create a new and smart analytics solution on top of the available state-of-the-art technolgies to open all the information of their data so they can providee the optimum customer experiences to the US visitors.\n",
    "\n",
    "## Structure of the Project\n",
    "Following the Udacity guide for this capstone, we structured the documentation of the project with the follwoing steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the Project and Gather Data\n",
    "\n",
    "_Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc_\n",
    "\n",
    "### The Scope \n",
    "The main goal of our project here will be to deliver a data warehouse in the cloud that will support the analytics team by answering questions through tables and dashboards. Moreover, we aim to built a source-of-truth database, so the Government of the US could open our smart solution through a web API so  they can query the DW for information relating to international  US visitors by backend web services.\n",
    "\n",
    "### The Data\n",
    "For this project we have used:\n",
    "- the immigration\n",
    "- the global temperature \n",
    "- demographics datasets and \n",
    "- the descriptions contained in the `I94_SAS_Labels_Descriptions.SAS` file.\n",
    "\n",
    "### The Architecture\n",
    "We provided a cloud based solution on top of __Amazon Web Services (AWS)__. First, we preprossed all the datasets provided with __Apache Spark__ and afterwards we stored the processed data in a staging area in __AWS S3__ bucket. Next, we loaded the staged data to a __Amazon Redshift__ cluster using an __Apache Airflow__ pipeline that transfer and check the quality of the data aiming to provide our customers a convenient data analysis.\n",
    "\n",
    "![Architecture](images/architecture.png)\n",
    "\n",
    "The main information that a user would like to extract from the data would be:\n",
    "\n",
    "* Visitors by nationality.\n",
    "* Visitors by airline.\n",
    "* Visitors by origin.\n",
    "* Correlations between destination in the U.S and the source country.\n",
    "* Correlations between visitor demographics, and states visited.\n",
    "* Correlations between immigration by source region, and the source region temperature.\n",
    "* Correlations between destination in the U.S and source climates.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Explore and Assess the Data\n",
    "\n",
    "_In order to get  familiar with the data provided by Udacity we performed a thorough exploratory data analysis ([EDA](https://en.wikipedia.org/wiki/Exploratory_data_analysis)) investigating the usefulness of the data, the potential preprocessing steps in order to clean, organize and join the provided datasets in a meaningful data model._\n",
    "\n",
    "We briefly describe the datasets provided in the following sections and we summarize the main reasons we had to consider when deciding what data we will use.\n",
    "\n",
    "__Immigration Data__\n",
    "\n",
    "For the last decades, U.S. immigration officers asked every foreign visitor who lawfully entered the United States (e.g.foreign students, business visitors and tourists) to submit the I-94 Form (Arrival/Departure Record). The cabin crew on arrival flights provided the foreign visitor with the I-94 was a small white paper and from the U.S. Customs and Border Protection at the time of entry into the United States. The form had a unique 11-digit identifying number assigned to it and listed port of entry, data of entry into the United States, the traveler's immigration category, the immigration status expiration date. The main purpose of that list was to record the visitor's lawful admission to the United States.\n",
    "\n",
    "The main dataset consists of one file for each month of the year of 2016 and it's available in the directory `../../data/18-83510-I94-Data-2016/` in the [SAS](https://www.sas.com/en_us/home.html) binary database storage format `sas7bdat`. Due to the massive ammount of data we had when we combined the 12 datasets (e.g. more than 40 million rows (40.790.529) and 28 columns), we used only the month of April of 2016 in our project which pproximately three million records (3.096.313)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing the libraries needed in this project\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "immigration = pd.read_sas(immigration_fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Data Dictionary__: Here, we  provide a description on the fields of the dataset. Some assumptions had to be made about the meaning descriptions as there were not clear enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| CICID* | Unique ID that identifies one record in the dataset |\n",
    "| I94YR | 4 digit numeric year |\n",
    "| I94MON | Numeric month |\n",
    "| I94CIT | 3 digit code of source city for immigration (Born country) |\n",
    "| I94RES | 3 digit code of source country for immigration (Residence country) |\n",
    "| I94PORT | Port addmitted through |\n",
    "| ARRDATE | Arrival date in the USA |\n",
    "| I94MODE | Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) |\n",
    "| I94ADDR | State of arrival |\n",
    "| DEPDATE | Departure date |\n",
    "| I94BIR | Age of Respondent in Years |\n",
    "| I94VISA | Visa codes collapsed into three categories: (1 = Business; 2 = Pleasure; 3 = Student) |\n",
    "| COUNT | Used for summary statistics |\n",
    "| DTADFILE | Character Date Field |\n",
    "| VISAPOST | Department of State where where Visa was issued |\n",
    "| OCCUP | Occupation that will be performed in U.S. |\n",
    "| ENTDEPA | Arrival Flag. Whether admitted or paroled into the US |\n",
    "| ENTDEPD | Departure Flag. Whether departed, lost visa, or deceased |\n",
    "| ENTDEPU | Update Flag. Update of visa, either apprehended, overstayed, or updated to PR |\n",
    "| MATFLAG | Match flag |\n",
    "| BIRYEAR | 4 digit year of birth |\n",
    "| DTADDTO | Character date field to when admitted in the US |\n",
    "| GENDER | Gender |\n",
    "| INSNUM | INS number |\n",
    "| AIRLINE | Airline used to arrive in U.S. |\n",
    "| ADMNUM | Admission number, should be unique and not nullable |\n",
    "| FLTNO | Flight number of Airline used to arrive in U.S. |\n",
    "| VISATYPE | Class of admission legally admitting the non-immigrant to temporarily stay in U.S. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The immigration dataset will be our fact meaning it will be at the center of the star schema model of our data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Global Temperature Data__\n",
    "\n",
    "The three most cited ocean and land temperature data sets provided to public are NOAA’s MLOST, NASA’s GISTEMP and the UK’s HadCrut.\n",
    "\n",
    "The Berkeley Earth is an organization affiliated with Lawrence Berkeley National Laboratory and recently has repackaged the data from a newer compilation and merged it all together. They have combined 1.6 billion temperature reports from 16 pre-existing archives. Their nicely packaged data allow the user to slice it into interesting subsets (for instance by country).  Their methodology allows weather observations from shorter periods to be included, so fewer observations are neededto be excluded. Finally, they provide the source code and data for all the transformations they have applied.\n",
    "\n",
    "The original dataset can be downoladed from [Kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data),and  even though several files are available, in our  capstone project we usedonly the `GlobalLandTemperaturesByCity`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "global_temperature = pd.read_csv(temperature_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Data Dictionary__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| dt | Date in format YYYY-MM-DD |\n",
    "| AverageTemperature | Average temperature of the city in a given date |\n",
    "| City | City Name |\n",
    "| Country | Country Name |\n",
    "| Latitude | Latitude |\n",
    "| Longitude | Longitude |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we can see, a long period of the world's temperature is provided at the dataset specifically from the year 1743 to the year 2013. However, we have been privided with the immigration dataset that contains data data of the US National Tourism Office only for the year of 2016, so the majority of the temperature data here seems not to be necessary. Afterwards, we aggregated this dataset by country, reported the average of the temperatures and use the resulting reduced table to join with lookup\\I94CIT_I94RES.csv lookup table, which was extracted from I94_SAS_Labels_Descriptions.SAS. The result of our preprocess is the COUNTRY dimension of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "global_temperature = global_temperature.groupby([\"Country\"]).agg({\"AverageTemperature\": \"mean\", \n",
    "                                                                        \"Latitude\": \"first\", \"Longitude\": \"first\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>13.816497</td>\n",
       "      <td>36.17N</td>\n",
       "      <td>69.61E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>15.525828</td>\n",
       "      <td>40.99N</td>\n",
       "      <td>19.17E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>17.763206</td>\n",
       "      <td>36.17N</td>\n",
       "      <td>3.98E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Angola</td>\n",
       "      <td>21.759716</td>\n",
       "      <td>12.05S</td>\n",
       "      <td>13.15E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>16.999216</td>\n",
       "      <td>39.38S</td>\n",
       "      <td>62.43W</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Country  AverageTemperature Latitude Longitude\n",
       "0  Afghanistan           13.816497   36.17N    69.61E\n",
       "1      Albania           15.525828   40.99N    19.17E\n",
       "2      Algeria           17.763206   36.17N     3.98E\n",
       "3       Angola           21.759716   12.05S    13.15E\n",
       "4    Argentina           16.999216   39.38S    62.43W"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_temperature.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Airports Data__\n",
    "\n",
    "The airport codes may refer to  either the [ICAO](https://en.wikipedia.org/wiki/ICAO_airport_code) airport code, a four letter code used by ATC systems and mainly for airports that do not have an IATA airport code (from wikipedia) or ti the [IATA](https://en.wikipedia.org/wiki/IATA_airport_code) airport code, a three-letter code which is used in passengers tickets and reservation, as well as the baggage-handling systems.\n",
    "\n",
    "We downoalde  the airports codes around the world from open source http://ourairports.com/data/ who collected this data from different sources.\n",
    "\n",
    "The list of all the airport codes is saved in the `airport-codes.csv`. Every attribute is identified in datapackage description. The columns of the dataset contain attributes, codes (IATA, local if exist) that are relevant to identification of an airport.\n",
    "The url of the original is http://ourairports.com/data/airports.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "airport = pd.read_csv(\"./sample_data/airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Data Dictionary__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| ident | Unique identifier |\n",
    "| type | Type of the airport |\n",
    "| name | Airport Name |\n",
    "| elevation_ft | Altitude of the airport |\n",
    "| continent | Continent |\n",
    "| iso_country | ISO code of the country of the airport |\n",
    "| iso_region | ISO code for the region of the airport |\n",
    "| municipality | City where the airport is located |\n",
    "| gps_code | GPS code of the airport |\n",
    "| iata_code | IATA code of the airport |\n",
    "| local_code | Local code of the airport |\n",
    "| coordinates | GPS coordinates of the airport |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As the aiport dataset has been proved a not so good source of analysis since we were not able to join it to the main table immigration, we decided not to use it in our model. None of the codes (ident, iata_code, gps_code or local_code) seemed to match the columns in the immigration fact table hence we did not find a consistent and valid key in both tables in order to cross them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__U.S. City Demographic Data__\n",
    "\n",
    "The following dataset contains demographics information about all the census-designated US cities with a population greater or equal to 65,000 citizents. This data are provided from the US Census Bureau's 2015 American Community Survey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "us_demographics = pd.read_csv(\"./sample_data/us-cities-demographics.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "__Data Dictionary__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| City | Name of the city |\n",
    "| State | US state of the city |\n",
    "| Median Age | The median of the age of the population |\n",
    "| Male Population | Number of the male population |\n",
    "| Female Population | Number of the female population |\n",
    "| Total Population | Number of the total population |\n",
    "| Number of Veterans | Number of veterans living in the city |\n",
    "| Foreign-born | Number of residents of the city that were not born in the city |\n",
    "| Average Household Size | Average size of the houses in the city |\n",
    "| State Code | Code of the state of the city |\n",
    "| Race | Race class |\n",
    "| Count | Number of individual of each race |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The main source of the STATE dimension table in our data model is the the `US Cities Demographics`. We aggregated the dataset by State and pivoted the `Count` and `Race`  columns as a result every different value of Race appears in a single column. A complete table of statistics has been created with the above method so all the information for every US state has been summarised successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the Data Model\n",
    "\n",
    "_In this section of the documentation we explaine the main process of our ETL pipeline, how we  extract, transform and load the data from the various datasets. Only 3-4 datasets provided by Udacity have been used in this project, namely: \n",
    "- immigration, \n",
    "- temperatures \n",
    "- demographics\n",
    "- the descriptions from labels descriptions file `I94_SAS_Labels_Descriptions.SAS`_\n",
    "\n",
    "#### 3.1 Conceptual Data Model\n",
    "_Map out the conceptual data model and explain why you chose that model_\n",
    "\n",
    "The center of our data moodel is essentially the immigration dataset. The main goal of this priject is to analyse the U.S visitors from the world and the immigration dataset is clearly the facts of what we want to analyse. We focused on this data mostly in our modeling phase. The DATE dimension table was constracted from the immigration dataset as well. We extracted all the distinct values of the arrival dates and departure dates of the columns `arrdate` and `depdate` and with the help of various functions we were able to save a the table of numerous attributes of a specific date: day, year,month, week of year and day of week.\n",
    "\n",
    "\n",
    "The aggregation of the demographics dataset by the State column resulted in the STATE dimension table. First, we aggregate the Female Population, Male Population, Total Population, Median Age, Number of Veterans, Foreign-born by `City` using `first` function, as they are duplicated accross the different rows of the same city. \n",
    "Next, the resulting rows were grouped by `State` applying the `sum` function in the numeric columns so we can privide the total in each U.S State. We used the pivot function of the `pyspark` packag to transform the `Race` column and create different columns for each different value. The final structure consisted of the columns:\n",
    "- BlackOrAfricanAmerican \n",
    "- White \n",
    "- ForeignBorn \n",
    "- AmericanIndianAndAlaskaNative\n",
    "- HispanicOrLatino\n",
    "- Asian\n",
    "- NumberVeterans\n",
    "- FemalePopulation\n",
    "- MalePopulation\n",
    "- TotalPopulation\n",
    "\n",
    "for each of the states of the U.S.\n",
    "\n",
    "The COUNTRY dimention completes our star schema data model. We combined the `GlobalLandTemperaturesByCity` with the code-descriptions found in the file `I94_SAS_Labels_Descriptions.SAS` for the columns `i94cit` and `i94res`. \n",
    "\n",
    "Firstly, we used the file `I94_SAS_Labels_Descriptions.SAS` to extracted the key-value pairs and saved those values in csv files in the `lookup` directory. Afterwards, we aggregated the temperature dataset first by `City` and then by `Country`. Finally, we join the two intermediary results to construct the table COUNTRY. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "_List the steps necessary to pipeline the data into the chosen data model_\n",
    "\n",
    "The package `helper.etl` contains all the main functions used to preprocess the datasets. There are different helper functions to load, clean, select, transform and save the resulting datasets in a very smart and convenient way. Our main tool was the open-source framework Apache Spark. Using Spark we have a strong easy to use interface for programming entire clusters with data parallelism and fault tolerance.\n",
    "\n",
    "Here we present only the general steps of the ETL since all the logic of preprocessing is located in the `helper.etl` package. \n",
    "\n",
    "We use this notebook here is only for document purposes. The  actual run of the ETL pipeline takes place in the Spark clouddata platform [Amazon EMR](https://aws.amazon.com/emr/?nc1=h_ls) through the execution of the main functions of the `etl` package. All the functions are documented with docstrings alongside the code of the package in `helper/etl.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Immigration and Date datasets\n",
    "The preprocessing of our  main dataset immigration starts by loading the data from the SAS file and afterwards we generate and store the processed dataframes to a S3 bucket in Amazon. In summary, our process consists of the following tasks:\n",
    "* Load of the immigration SAS file into Spark dataframe. We only include the useful columns identified  in the EDA phase. Specifically we discarded the follouwing columns: 'admnum', 'biryear', 'count', 'dtaddto', 'dtadfile', 'entdepa', 'entdepd', 'entdepu', 'insnum', 'matflag', 'occup', 'visapost';\n",
    "* We converted the columns of Integer type to the proper class, as the Spark framework loaded them as double or strings.\n",
    "* The dates are stored in SAS date format in the immigration dataframe,  e.g. a value that represents the number of days between a specified date and January 1, 1960. We convert the dates in the dataframe to a string date format YYYY-MM-DD;\n",
    "* We drop the columns \"visapost\", \"occup\", \"entdepu\" and \"insnum\" as they were missing a vast ammount of values;\n",
    "* We created the `stay` column by calculating the difference in days between the departure (depdate) and arrival (arrdate) date of the US visitors. In this way, it will be easier for our users to analyse the lenght of the average stay of visitors and where they aim to stay longer;\n",
    "* We created the DATE dataframe from thedate columns `arrdate` and `depdate` ;\n",
    "* We saved the processed `immigration` and `date` dataframes in parquet format in the Amazon S3 bucket;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import the ETL package\n",
    "import os\n",
    "import os, re\n",
    "import configparser\n",
    "import logging\n",
    "from datetime import timedelta, datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import udf, col, when, lower, isnull, year, month, dayofmonth, hour, weekofyear, dayofweek, date_format, to_date\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, DoubleType\n",
    "from pyspark.sql.types import *\n",
    "from helper.etl import create_spark_session, process_immigration_data, \\\n",
    "process_temperature_data, process_airport_data, \\\n",
    "process_demographics_data, process_states_data, process_countries_data\n",
    "from helper.etl import convert_SAS_date, load_data, save_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The configuration file \"dl.cfg\" includs the AWS key id and secret access key\n",
    "config = configparser.ConfigParser()\n",
    "config.read('./helper/dl.cfg')\n",
    "#The AWS access key information are saved in  environment variables\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform ETL process for the Immigration dataset generating immigration and date tables and save them in the S3 bucket indicated in the output_path parameters.\n",
    "immigration = process_immigration_data(spark, input_path='../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat',\n",
    "                                           #output_path=\"s3a://udacity-data-engineer-capstone/immigration.parquet\",\n",
    "                                       output_path=None,\n",
    "                                           date_output_path=\"s3a://udacity-data-engineer-capstone/date.parquet\",\n",
    "                                           input_format = \"com.github.saurfang.sas.spark\", \n",
    "                                           columns = ['i94addr', 'i94mon','cicid','i94visa','i94res','arrdate','i94yr','depdate',\n",
    "                                                      'airline', 'fltno', 'i94mode', 'i94port', 'visatype', 'gender', \n",
    "                                                      'i94cit', 'i94bir'],\n",
    "                                           debug_size=1000, partitionBy = [\"i94yr\", \"i94mon\"], saved_columns = '*', header=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Countries dataset\n",
    "To generate the country dataset firts we need to load the data global temperature dataset as well as I94CIT_I94RES lookup table. Afterwards, we generated and stored of the processed dataframe to a S3 bucket in Amazon. In summary, we performed the following tasks :\n",
    "* We loaded of the global temperature csv file and the I94CIT_I94RES lookup table;\n",
    "* We aggregated the temperatures dataset by country and rename the new columns;\n",
    "* We joined the two resalted datasets;\n",
    "* We saved the wto dataset to the staging area in the S3 Amazon bucket;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "countries = process_countries_data(spark, input_path=\"../../data2/GlobalLandTemperaturesByCity.csv\",\n",
    "                                   output_path=\"s3a://udacity-data-engineer-capstone/country.parquet\",\n",
    "                                   input_format = \"csv\", columns = '*', debug_size = None,\n",
    "                                   header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### States dataset\n",
    "We need to load the data in the demographics dataset so we can obtain the states codes, as well as the I94ADDR lookup table to generate the states dataset. Afterwards, to complete the generation we need to store the processed dataframe to a S3 bucket in Amazon. In summary, performed these steps in our  process:\n",
    "* We run the process_demograohics_data functioon to optain the states codes.\n",
    "* We loaded I94ADDR lookup table;\n",
    "* We aggregated of the demographics dataset by state and renamesd the new columns;\n",
    "* We joined the two datasets;\n",
    "* We saved the resulting dataset to the staging area in the S3 bucket in Amazon;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographics = process_demographics_data(spark, input_path=\"./sample_data/us-cities-demographics.csv\",\n",
    "                                          output_path=\"s3a://udacity-data-engineer-capstone/demographics.parquet\",\n",
    "                                          input_format = \"csv\", columns='*',\n",
    "                                          debug_size = None, partitionBy = [\"State Code\"],\n",
    "                                          header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "cols = ['TotalPopulation', 'FemalePopulation', 'MalePopulation', 'NumberVeterans', 'ForeignBorn', \n",
    "            'AmericanIndianAndAlaskaNative', 'Asian', 'BlackOrAfricanAmerican', 'HispanicOrLatino', 'White']\n",
    "states_codes = demographics.groupby([\"State Code\", \"State\"]).agg(dict(zip(cols, len(cols)*[\"sum\"])))\n",
    "states = process_states_data(spark, input_path=\"./lookup/lookup/I94ADDR.csv\", output_path=\"s3a://udacity-data-engineer-capstone/states.parquet\",\n",
    "                             input_format = \"csv\", columns= ['TotalPopulation', 'FemalePopulation', 'MalePopulation',\n",
    "                                                             'NumberVeterans', 'ForeignBorn',\n",
    "                                                             'AmericanIndianAndAlaskaNative', 'Asian', 'BlackOrAfricanAmerican',\n",
    "                                                             'HispanicOrLatino', 'White'],\n",
    "                             debug_size = None, partitionBy = [\"State Code\"], states_codes = states_codes, \n",
    "                             header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "_Build the data pipelines to create the data model._\n",
    "\n",
    "The whole ETL pipeline is divided into two stages:\n",
    "1) The first one, where we used Spark to load, extract, transform and save the provided datasets into the S3 bucket in AWS (staging area).\n",
    "2) The second stage, where we used the [Apache Airflow](https://airflow.apache.org/) to build a DAG to extract data from S3 buckets and load them into tables with the same name in Amazon Redshift. Finally we perfomerd data qualitu checks to ensure completeness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "<img src=\"images/architecture.png\" alt=\"architecture\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Here you can see the second stage we builted using Apache Airflow.\n",
    "\n",
    "<img src=\"images/dag.PNG\" alt=\"dag\"  width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The folder `airflow` contains the code to built the airflow pipeline. Inside the `dags` folder you will find the code of the DAG `udacity_capstone.py` as well as the three custom operators built folder `plugins/operators`: `create_tables.oy`, `stage_redshift.py` and `data_quality.py`.\n",
    "\n",
    "The custom operator `CreateTablesOperator` was designed to create tables in the AWS Redshift. The custom operator `StageToRedshiftOperator` was designed to load the data in [parquet](https://parquet.apache.org/) format from the S3 buckets in AWS and insert the content of the data files into the created tables in AWS Redshift. \n",
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "After completing the loading steps, we perform some data quality checks with the help of the custom operator `DataQualityOperator` to make sure everything was sucessfull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "_Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file._\n",
    "\n",
    "\n",
    "__Table Immigration__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| CICID | Primary Key |\n",
    "| I94CIT | 3 digit for the country code where the visitor was born. This is a FK to the COUNTRY dimension table |\n",
    "| I94RES | 3 digit for the country code where the visitor resides in. This is a FK to the COUNTRY dimension table |\n",
    "| ARRDATE | Arrival date in the USA. This is a FK to the DATE dimension table |\n",
    "| I94MODE | Mode of transportation (1 = Air; 2 = Sea; 3 = Land; 9 = Not reported) |\n",
    "| I94ADDR | State of arrival. This is a FK to the STATE dimension table |\n",
    "| DEPDATE | Departure date from the USA. This is a FK to the DATE dimension table |\n",
    "| I94BIR | Age of Respondent in Years |\n",
    "| I94VISA | Visa codes collapsed into three categories: (1 = Business; 2 = Pleasure; 3 = Student) |\n",
    "| BIRYEAR | 4 digit year of birth |\n",
    "| GENDER | Gender |\n",
    "| AIRLINE | Airline used to arrive in U.S. |\n",
    "| FLTNO | Flight number of Airline used to arrive in U.S. |\n",
    "| VISATYPE | Class of admission legally admitting the non-immigrant to temporarily stay in U.S. |\n",
    "| STAY | Number of days in the US |\n",
    "\n",
    "\n",
    "__Table STATE__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| Code | Primary Key. This is the code of the State as in I94ADDR lookup table |\n",
    "| State | Name of the state |\n",
    "| BlackOrAfricanAmerican | Number of residents of the race Black Or African American |\n",
    "| White | Number of residents of the race White |\n",
    "| ForeignBorn | Number of residents that born outside th United States |\n",
    "| AmericanIndianAndAlaskaNative | Number of residents of the race American Indian And Alaska Native |\n",
    "| HispanicOrLatino | Number of residents of the race Hispanic Or Latino |\n",
    "| Asian | Number of residents of the race Asian |\n",
    "| NumberVeterans | Number of residents that are war veterans |\n",
    "| FemalePopulation | Number of female population |\n",
    "| MalePopulation | Number of male population |\n",
    "| TotalPopulation | Number total of the population |\n",
    "\n",
    "\n",
    "__Table COUNTRY__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| Code | Country Code. This is the PK. |\n",
    "| Country | Country Name |\n",
    "| Temperature | Average temperature of the country between 1743 and 2013 |\n",
    "| Latitude | GPS Latitude |\n",
    "| Longitude | GPS Longitude |\n",
    "\n",
    "\n",
    "__Table DATE__\n",
    "\n",
    "| Column Name | Description |\n",
    "| :--- | :--- |\n",
    "| date | Date in the format YYYY-MM-DD. This is the PK. |\n",
    "| day | Two digit day |\n",
    "| month | Two digit month |\n",
    "| year | Four digit for the year |\n",
    "| weekofyear | The week of the year |\n",
    "| dayofweek | The day of the week |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "__Clearly state the rationale for the choice of tools and technologies for the project.__\n",
    "\n",
    "The whole project and our solution is launched on cloud computing technology, specifically the AWS. The cloud computing and AWS provides a reliable, easy to use, scalable and low-cost infrastructure platform in the cloud, the choise for every new solution is very simple. The cost of every service used was reasonable and it was ‘pay as you go’ pricing, a fact that help us start small and scale our solution as we grow.\n",
    "\n",
    "In particular, why we use the following services:\n",
    "\n",
    "__S3:__ Provides an easy-to-use, scalable and low cost staging area with optimum availability, security, and performance;\n",
    "\n",
    "__Spark:__ Spark is a very strong framework for big data processing, with various built-in modules for streaming, SQL quering, data/graph processing and machine learning. An whole interface for programming clusters with implicit data parallelism and fault tolerance is provided by Spark.;\n",
    "\n",
    "__EMR:__ EMR is a cloud big data platform, which allows team members to process huge ammounts of data in no time, and it is cost-effectively at scale using Spark. EMR is seacure, elastic, low cost and friendly for users. Ideal for our DW project;\n",
    "\n",
    "__Redshift:__  Redshift provides a parallel, column-oriented data warehouse with easy-scale functionality, facts that made our choise very easy.\n",
    "\n",
    "\n",
    "__Propose how often the data should be updated and why__\n",
    "\n",
    "The data model will be updated montly since we receive one file per month. \n",
    "\n",
    "\n",
    "__Write a description of how you would approach the problem differently under the following scenarios:__\n",
    "\n",
    " * The data was increased by 100x:\n",
    "\n",
    "Our whole solution is cloud based, specifically in AWS, so scalling the pipeline should not create any issues. \n",
    "By increasing the number of nodes of EMR clusters we can hadle more data, as the Amazon Redshift is a data warehouse that can expand to exabyte-scale;\n",
    " \n",
    "* The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "We can adjust the running interval of the Airflow DAG to `daily` and schedul the run overnight so we will be able to provide the data by 7am.\n",
    " \n",
    "* The database needed to be accessed by 100+ people.\n",
    "\n",
    "In Redshift, there is a usefull feature `elastic resize` which allows us to add or remove node in a cluster in Amazon Reshift within minutes. \n",
    "We can easily obtain extra storage and better performance according to our needs and the  demands of our workloads, and we can easilu reduce the cost during the periods where the demand is relatively low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
